#!/usr/bin/env ruby
require 'gli'
require 'curly'
require 'curly/curl'

include GLI::App

program_desc 'A scriptable interface to curl'

version Curly::VERSION

subcommand_option_handling :normal
arguments :strict

ImageType = Class.new(Symbol)
accept ImageType do |value|
  value.normalize_image_type(:all)
end

BrowserType = Class.new(Symbol)
accept BrowserType do |value|
  value.normalize_browser_type(:none)
end

ScreenshotType = Class.new(Symbol)
accept ScreenshotType do |value|
  value.normalize_screenshot_type(:full_page)
end

desc 'Output YAML instead of json'
switch %i[y yaml]

desc 'Output "pretty" JSON'
switch %i[pretty], default_value: true, negatable: true

# TODO: Post method, html and json with --data flags, accept key=value and files
# TODO: Handle binary responses, deal gracefully with compressed data
# TODO: File uploads?

def self.break_headers(headers)
  out = {}
  headers.each do |h|
    m = h.match(/(?<key>[^=]+)=(?<value>.*?)$/)
    out[m['key'].strip] = m['value'].strip
  end
  out
end

def self.print_out(output, yaml, raw: false, pretty: true)
  output = output.to_data if output.respond_to?(:to_data)
  # Was intended to flatten single responses, but not getting an array back is unpredictable
  output = output.clean_output
  if output.is_a?(String)
    print output
  elsif raw
    output = output.join("\n") if output.is_a?(Array)
    print output
  else
    if yaml
      print YAML.dump(output)
    else
      print pretty ? JSON.pretty_generate(output) : JSON.generate(output)
    end
  end
end

desc 'Curl URL and output its elements, multiple URLs allowed'
arg_name 'URL', multiple: true
command %i[html curl] do |c|
  c.desc 'Only retrieve headers/info'
  c.switch %i[I info], negatable: false

  c.desc 'Regurn an array of matches to a CSS or XPath query'
  c.flag %i[s search]

  c.desc 'Define a header to send as "key=value"'
  c.flag %i[h header], multiple: true

  c.desc 'Use a browser to retrieve a dynamic web page (firefox, chrome)'
  c.flag %i[b browser], type: BrowserType, must_match: /^[fc].*?$/

  c.desc %(If curl doesn't work, use a fallback browser (firefox, chrome))
  c.flag %i[f fallback], type: BrowserType, must_match: /^[fc].*?$/

  c.desc 'Expect compressed results'
  c.switch %i[c compressed], negatable: false

  c.desc 'Remove extra whitespace from results'
  c.switch %i[clean]

  c.desc 'Filter output using dot-syntax path'
  c.flag %i[q query filter]

  c.desc 'Output a raw value for a key'
  c.flag %i[r raw]

  c.desc 'Ignore relative hrefs when gathering content links'
  c.switch %i[ignore_relative], negatable: true

  c.desc 'Ignore fragment hrefs when gathering content links'
  c.switch %i[ignore_fragments], negatable: true

  c.desc 'Only gather external links'
  c.switch %i[x external_links_only], default_value: false, negatable: false

  c.desc 'Only gather internal (same-site) links'
  c.switch %i[l local_links_only], default_value: false, negatable: false

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)
    headers = break_headers(options[:header])

    output = []

    urls.each do |url|
      curl_settings = { browser: options[:browser], fallback: options[:fallback],
                        headers: headers, headers_only: options[:info],
                        compressed: options[:compressed], clean: options[:clean],
                        ignore_local_links: options[:ignore_relative],
                        ignore_fragment_links: options[:ignore_fragments],
                        external_links_only: options[:external_links_only],
                        local_links_only: options[:local_links_only] }
      res = Curl::Html.new(url, curl_settings)
      res.curl

      if options[:info]
        output.push(res.headers)
        next
      end

      if options[:search]
        source = res.search(options[:search], return_source: true)

        out = res.parse(source)

        if options[:query]
          out = out.to_data(url: url, clean: options[:clean]).dot_query(options[:query], full_tag: false)
        else
          out = out.to_data
        end
        output.push([out])
      elsif options[:query]
        queried = res.to_data.dot_query(options[:query], full_tag: false)
        output.push(queried) if queried
      else
        output.push(res.to_data(url: url))
      end
    end
    output.delete_if(&:nil?)
    output.delete_if(&:empty?)

    exit_now!('No results') if output.nil? || output.empty?

    output.map! { |o| o[options[:raw].to_sym] } if options[:raw]

    output = output.clean_output

    print_out(output, global_options[:yaml], raw: options[:raw], pretty: global_options[:pretty])
  end
end

desc 'Execute JavaScript on a URL'
arg_name 'URL', multiple: true
command :execute do |c|
  c.desc 'Browser to use (firefox, chrome)'
  c.flag %i[b browser], type: BrowserType, must_match: /^[fc].*?$/, default_value: 'chrome'

  c.desc 'Define a header to send as key=value'
  c.flag %i[h header], multiple: true

  c.desc 'Script to execute, use - to read from STDIN'
  c.flag %i[s script], multiple: true

  c.desc 'Element ID to wait for before executing'
  c.flag %i[i id]

  c.desc 'Seconds to wait after executing JS'
  c.flag %i[w wait], default_value: 2

  c.action do |_, options, args|
    urls = args.join(' ').split(/[, ]+/)

    raise 'Script input required' unless options[:file] || options[:script]

    compiled_script = []

    if options[:script].count.positive?
      options[:script].each do |scr|
        scr.strip!
        if scr == '-'
          compiled_script << $stdin.read
        elsif File.exist?(File.expand_path(scr))
          compiled_script << IO.read(File.expand_path(scr))
        else
          compiled_script << scr
        end
      end
    end

    script = compiled_script.count.positive? ? compiled_script.join(';') : nil

    headers = break_headers(options[:header])

    browser = options[:browser]

    browser = browser.is_a?(Symbol) ? browser : browser.normalize_browser_type

    urls.each do |url|
      c = Curl::Html.new(url)
      c.headers = headers
      c.browser = browser
      $stdout.puts c.execute(script, options[:wait], options[:id])
    end
  end
end

desc 'Save a screenshot of a URL'
arg_name 'URL', multiple: true
command :screenshot do |c|
  c.desc 'Type of screenshot to save (full (requires firefox), print, visible)'
  c.flag %i[t type], type: ScreenshotType, must_match: /^[fpv].*?$/, default_value: 'visible'

  c.desc 'Browser to use (firefox, chrome)'
  c.flag %i[b browser], type: BrowserType, must_match: /^[fc].*?$/, default_value: 'chrome'

  c.desc 'File destination'
  c.flag %i[o out file], required: true

  c.desc 'Define a header to send as key=value'
  c.flag %i[h header], multiple: true

  c.desc 'Script to execute before taking screenshot'
  c.flag %i[s script], multiple: true

  c.desc 'Element ID to wait for before taking screenshot'
  c.flag %i[i id]

  c.desc 'Time to wait before taking screenshot'
  c.flag %i[w wait], default_value: 0, type: Integer

  c.action do |_, options, args|
    urls = args.join(' ').split(/[, ]+/)
    headers = break_headers(options[:header])

    type = options[:type]
    browser = options[:browser]

    type = type.is_a?(Symbol) ? type : type.normalize_screenshot_type
    browser = browser.is_a?(Symbol) ? browser : browser.normalize_browser_type

    compiled_script = []

    if options[:script].count.positive?
      options[:script].each do |scr|
        scr.strip!
        if scr == '-'
          compiled_script << $stdin.read
        elsif File.exist?(File.expand_path(scr))
          compiled_script << IO.read(File.expand_path(scr))
        else
          compiled_script << scr
        end
      end
    end

    script = compiled_script.count.positive? ? compiled_script.join(';') : nil

    raise 'Full page screen shots only available with Firefox' if type == :full_page && browser != :firefox

    urls.each do |url|
      c = Curl::Html.new(url)
      c.headers = headers
      c.browser = browser
      c.screenshot(options[:out], type: type, script: script, id: options[:id], wait: options[:wait])
    end
  end
end

desc 'Get a JSON response from a URL, multiple URLs allowed'
arg_name 'URL', multiple: true
command :json do |c|
  c.desc 'Define a header to send as key=value'
  c.flag %i[h header], multiple: true

  c.desc 'Expect compressed results'
  c.switch %i[c compressed]

  c.desc 'Filter output using dot-syntax path'
  c.flag %i[q query filter]

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)
    headers = break_headers(options[:header])

    output = []

    urls.each do |url|
      res = Curl::Json.new(url)
      res.request_headers = headers
      res.compressed = options[:compressed],
      res.symbolize_names = false
      res.curl

      json = res.json

      if json.nil?
        output.push({
          status: 'error parsing JSON',
          url: res.url,
          code: res.code,
          headers: res.headers
        })
      else
        if options[:query]
          if options[:query] =~ /^json$/
            res = json
          elsif options[:query] =~ /^json\./
            query = options[:query].sub(/^json\./, '')
          else
            query = options[:query]
          end

          res = json.dot_query(query)
        else
          res = res.to_data
        end

        output.push(res)
      end
    end

    output = output.clean_output

    print_out(output, global_options[:yaml], pretty: global_options[:pretty])
  end
end

desc 'Extract contents between two regular expressions'
arg_name 'URL', multiple: true
command :extract do |c|
  c.desc 'Text before extraction'
  c.flag %i[b before]

  c.desc 'Text after extraction'
  c.flag %i[a after]

  c.desc 'Process before/after strings as regular expressions'
  c.switch %i[r regex]

  c.desc 'Include the before/after matches in the result'
  c.switch %i[i include]

  c.desc 'Define a header to send as key=value'
  c.flag %i[h header], multiple: true

  c.desc 'Expect compressed results'
  c.switch %i[c compressed]

  c.desc 'Remove extra whitespace from results'
  c.switch %i[clean]

  c.desc 'Strip HTML tags from results'
  c.switch %i[strip]

  c.action do |global_options,options,args|
    urls = args.join(' ').split(/[, ]+/)
    headers = break_headers(options[:header])

    output = []

    urls.each do |url|
      res = Curl::Html.new(url, { headers: headers, headers_only: false,
                                  compressed: options[:compressed], clean: options[:clean] })
      res.curl
      if options[:regex]
        before = Regexp.new(options[:before])
        after = Regexp.new(options[:after])
      else
        before = /#{Regexp.escape(options[:before])}/
        after = /#{Regexp.escape(options[:after])}/
      end

      extracted = res.extract(before, after, inclusive: options[:include])
      extracted.strip_tags! if options[:strip]
      output.concat(extracted)
    end

    print_out(output, global_options[:yaml], pretty: global_options[:pretty])
  end
end

desc 'Extract all instances of a tag'
arg_name 'URL', multiple: true
command :tags do |c|
  c.desc 'Define a header to send as key=value'
  c.flag %i[h header], multiple: true, arg_name: 'KEY=VAL'

  c.desc 'Specify a tag to collect'
  c.flag %i[t tag], multiple: true, arg_name: 'TAG'

  c.desc 'Expect compressed results'
  c.switch %i[c compressed]

  c.desc 'Remove extra whitespace from results'
  c.switch %i[clean]

  c.desc 'Output the HTML source of the results'
  c.switch %i[source html]

  c.desc 'Dot syntax query to filter results'
  c.flag %i[q query filter], arg_name: 'DOT_SYNTAX'

  c.desc 'Regurn an array of matches to a CSS or XPath query'
  c.flag %i[search], arg_name: 'CSS/XPATH'

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)
    headers = break_headers(options[:header])
    tags = options[:tag].join(' ').split(/[, ]+/)
    output = []

    urls.each do |url|
      res = Curl::Html.new(url, { headers: headers, headers_only: options[:headers],
                                  compressed: options[:compressed], clean: options[:clean] })
      res.curl

      output = []
      if options[:search]
        out = res.search(options[:search])

        out = out.dot_query(options[:query]) if options[:query]
        output.push(out)
      elsif options[:query]
        output = res.to_data.dot_query(options[:query])
      elsif tags.count.positive?
        tags.each { |tag| output.concat(res.tags(tag)) }
      else
        output.concat(res.tags)
      end
    end

    output = output.clean_output

    if options[:source]
      puts output.to_html
    else
      print_out(output, global_options[:yaml], pretty: global_options[:pretty])
    end
  end
end

desc 'Extract all images from a URL'
arg_name 'URL', multiple: true
command :images do |c|
  c.desc 'Type of images to return (img, srcset, opengraph, all)'
  c.flag %i[t type], multiple: true, type: ImageType, default_value: ['all']

  c.desc 'Expect compressed results'
  c.switch %i[c compressed]

  c.desc 'Remove extra whitespace from results'
  c.switch %i[clean]

  c.desc 'Filter output using dot-syntax path'
  c.flag %i[q query filter]

  c.desc 'Define a header to send as key=value'
  c.flag %i[h header], multiple: true

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)
    headers = break_headers(options[:header])

    output = []

    types = options[:type].join(' ').split(/[ ,]+/).map(&:normalize_image_type)

    urls.each do |url|
      res = Curl::Html.new(url, { compressed: options[:compressed], clean: options[:clean] })
      res.curl

      res = res.images(types: types)
      res = { images: res }.dot_query(options[:query], 'images', full_tag: false) if options[:query]

      if res.is_a?(Array)
        output.concat(res)
      else
        output.push(res)
      end
    end

    exit_now!('No results') if output.nil? || output.empty?

    print_out(output, global_options[:yaml], pretty: global_options[:pretty])
  end
end

desc %(Return all links on a URL's page)
arg_name 'URL', multiple: true
command :links do |c|
  c.desc 'Ignore relative hrefs when gathering content links'
  c.switch %i[ignore_relative], negatable: true

  c.desc 'Ignore fragment hrefs when gathering content links'
  c.switch %i[ignore_fragments], negatable: true

  c.desc 'Only gather external links'
  c.switch %i[x external_links_only], default_value: false, negatable: false

  c.desc 'Only gather internal (same-site) links'
  c.switch %i[l local_links_only], default_value: false, negatable: false

  c.desc 'Filter output using dot-syntax path'
  c.flag %i[q query filter]

  c.desc 'Filter out duplicate links, preserving only first one'
  c.switch %i[d dedup], negatable: true

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)

    output = []

    urls.each do |url|
      res = Curl::Html.new(url, {
                             compressed: options[:compressed], clean: options[:clean],
                             ignore_local_links: options[:ignore_relative],
                             ignore_fragment_links: options[:ignore_fragments],
                             external_links_only: options[:external_links_only],
                             local_links_only: options[:local_links_only]
                           })
      res.curl

      if options[:query]
        queried = res.to_data.dot_query(options[:query], 'links', full_tag: false)

        queried.is_a?(Array) ? output.concat(queried) : output.push(queried) if queried
      else
        output.concat(res.body_links)
      end
    end

    output.dedup_links! if options[:dedup]

    print_out(output, global_options[:yaml], pretty: global_options[:pretty])
  end
end

desc %(Return all <head> links on URL's page)
arg_name 'URL', multiple: true
command :headlinks do |c|
  c.desc 'Filter output using dot-syntax path'
  c.flag %i[q query filter]

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)

    output = []

    urls.each do |url|
      res = Curl::Html.new(url, { compressed: options[:compressed], clean: options[:clean] })
      res.curl

      if options[:query]
        queried = { links: res.to_data[:meta_links] }.dot_query(options[:query], 'links', full_tag: false)
        output.push(queried) if queried
      else
        output.push(res.to_data[:meta_links])
      end
    end

    output = output.clean_output

    print_out(output, global_options[:yaml], pretty: global_options[:pretty])
  end
end

desc %(Scrape a page using a web browser, for dynamic (JS) pages. Be sure to have the selected --browser installed.)
arg_name 'URL', multiple: true
command :scrape do |c|
  c.desc 'Browser to use (firefox, chrome)'
  c.flag %i[b browser], type: BrowserType, required: true

  c.desc 'Regurn an array of matches to a CSS or XPath query'
  c.flag %i[search]

  c.desc 'Define a header to send as "key=value"'
  c.flag %i[h header], multiple: true

  c.desc 'Remove extra whitespace from results'
  c.switch %i[clean]

  c.desc 'Filter output using dot-syntax path'
  c.flag %i[q query filter]

  c.desc 'Output a raw value for a key'
  c.flag %i[r raw]

  c.action do |global_options, options, args|
    urls = args.join(' ').split(/[, ]+/)

    output = []

    urls.each do |url|
      res = Curl::Html.new(url, { browser: options[:browser], clean: options[:clean] })
      res.curl

      if options[:search]
        out = res.search(options[:search])

        out = out.dot_query(options[:query], full_tag: false) if options[:query]
        output.push(out)
      elsif options[:query]
        queried = res.to_data(url: url).dot_query(options[:query], full_tag: false)
        output.push(queried) if queried
      else
        output.push(res.to_data(url: url))
      end
    end

    output.delete_if(&:empty?)

    output = output.clean_output

    if options[:raw]
      output.map! { |o| o[options[:raw].to_sym] }
    end

    print_out(output, global_options[:yaml], raw: options[:raw], pretty: global_options[:pretty])
  end
end

pre do |global, command, options, args|
  # Pre logic here
  # Return true to proceed; false to abort and not call the
  # chosen command
  # Use skips_pre before a command to skip this block
  # on that command only
  true
end

post do |global, command, options, args|
  # Post logic here
  # Use skips_post before a command to skip this
  # block on that command only
end

on_error do |exception|
  # Error logic here
  # return false to skip default error handling
  true
end

exit run(ARGV)
